# Neural Network from Scratch: Pure Mathematics Implementation

## üß† Overview

This project implements a **complete neural network from scratch** using only fundamental mathematical operations and NumPy. No high-level machine learning frameworks like TensorFlow or PyTorch are used‚Äîjust pure mathematics, linear algebra, and calculus brought to life in code.

The implementation demonstrates the core mathematical principles behind deep learning, making it an excellent educational resource for understanding how neural networks actually work under the hood.

## üéØ Key Features

### Mathematical Foundation
- **Pure Implementation**: Built entirely from mathematical first principles
- **No ML Frameworks**: Uses only NumPy for numerical computations
- **Educational Focus**: Every function is thoroughly documented with mathematical explanations

### Supported Tasks
- **Classification**: Multi-class classification with softmax output
- **Regression**: Continuous value prediction with linear output
- **Flexible Architecture**: Configurable hidden layers and neuron counts

### Advanced Features
- **Multiple Weight Initialization Schemes**: He Normal and Xavier Normal
- **Diverse Activation Functions**: ReLU, Leaky ReLU, Sigmoid, Tanh, Softmax
- **Robust Loss Functions**: Cross-entropy for classification, MSE for regression
- **Gradient Descent Optimization**: Full backpropagation implementation
- **Model Persistence**: Best model tracking during training

## üî¨ Mathematical Concepts Implemented

### 1. Weight Initialization
**He Normal Initialization** (for ReLU-based networks):
```
W ~ N(0, ‚àö(2/fan_in))
```

**Xavier Normal Initialization** (for symmetric activations):
```
W ~ N(0, ‚àö(2/(fan_in + fan_out)))
```

### 2. Forward Propagation
For each layer `l`:
```
Z^(l) = W^(l) ¬∑ A^(l-1) + b^(l)
A^(l) = activation(Z^(l))
```

### 3. Activation Functions
- **ReLU**: `f(x) = max(0, x)`
- **Leaky ReLU**: `f(x) = max(0.01x, x)`
- **Sigmoid**: `f(x) = 1/(1 + e^(-x))`
- **Softmax**: `f(x_i) = e^(x_i) / Œ£(e^(x_j))`

### 4. Loss Functions
**Cross-Entropy** (Classification):
```
L = -1/m Œ£ Œ£ y_ij ¬∑ log(≈∑_ij)
```

**Mean Squared Error** (Regression):
```
L = 1/m Œ£ (y_i - ≈∑_i)¬≤
```

### 5. Backpropagation
Gradient computation using chain rule:
```
‚àÇL/‚àÇW^(l) = ‚àÇL/‚àÇA^(l) ¬∑ ‚àÇA^(l)/‚àÇZ^(l) ¬∑ ‚àÇZ^(l)/‚àÇW^(l)
```

## üìÅ Project Structure

```
Neural-Network/
‚îú‚îÄ‚îÄ neural_network_model.py   # Core neural network implementation
‚îú‚îÄ‚îÄ iris_classification.py    # Classification example with Iris dataset
‚îú‚îÄ‚îÄ housing_data_regression.py # Regression example with housing data
‚îú‚îÄ‚îÄ data/                     # Dataset folder
‚îú‚îÄ‚îÄ README.md                 # This documentation
‚îî‚îÄ‚îÄ LICENSE                   # MIT License
```

## üöÄ Quick Start

### Installation
```bash
# Clone the repository
git clone https://github.com/[your-username]/Neural-Network.git
cd Neural-Network

# Install required dependencies
pip install numpy pandas scikit-learn
```

### Basic Usage

#### Classification Example
```python
# Run the Iris classification example
python iris_classification.py

# Or import and use the neural network directly
from neural_network_model import *
import numpy as np
import pandas as pd
from sklearn.preprocessing import LabelEncoder

# Load your data
X = pd.DataFrame(your_features)
y = pd.Series(your_labels)

# One-hot encode labels
label_encoder = LabelEncoder()
y_encoded = label_encoder.fit_transform(y)
y_one_hot = np.eye(len(np.unique(y_encoded)))[y_encoded]

# Define network architecture
neuron_count = [64, 32, len(np.unique(y_encoded))]  # 2 hidden layers + output
hidden_layers = 2
epochs = 1000
learning_rate = 0.01

# Train the network
model = classification_network_training(
    X, y_one_hot, neuron_count, epochs, learning_rate, hidden_layers
)

# Make predictions
predictions = classification_prediction(X_test, model, hidden_layers)
```

#### Regression Example
```python
# Run the housing data regression example
python housing_data_regression.py

# Or import and use the neural network directly
from neural_network_model import *
import pandas as pd

# Load your data
X = pd.DataFrame(your_features)
y = pd.Series(your_targets)

# Define network architecture
neuron_count = [64, 32, 1]  # 2 hidden layers + 1 output neuron
hidden_layers = 2
epochs = 1000
learning_rate = 0.001

# Train the network
model = regression_network_training(
    X, y, neuron_count, epochs, learning_rate, hidden_layers
)

# Make predictions
predictions = regression_prediction(X_test, model, hidden_layers)
```

## üîß API Reference

### Core Functions

#### Weight Initialization
- `he_initialization(shape)`: He Normal initialization for ReLU networks
- `xavier_initialization(shape)`: Xavier Normal initialization for symmetric activations
- `initialize_biases(shape)`: Zero initialization for biases
- `initialize_biases_random(shape)`: Small random initialization for biases

#### Activation Functions
- `relu(x)`: Rectified Linear Unit
- `leaky_relu(x)`: Leaky ReLU with Œ±=0.01
- `sigmoid(x)`: Sigmoid activation
- `tanh(x)`: Hyperbolic tangent
- `softmax(matrix)`: Softmax for multi-class classification

#### Loss Functions
- `calculate_cross_entropy(actual, predicted)`: Cross-entropy loss
- `calculate_mse(actual, predicted)`: Mean squared error loss

#### Training Functions
- `classification_network_training(X, y_one_hot, neuron_count, epochs, learning_rate, hidden_layers)`: Train classification network
- `regression_network_training(X, y, neuron_count, epochs, learning_rate, hidden_layers)`: Train regression network

#### Prediction Functions
- `classification_prediction(X, model, hidden_layers)`: Make classification predictions
- `regression_prediction(X, model, hidden_layers)`: Make regression predictions

## ‚öôÔ∏è Configuration Options

### Network Architecture
- **neuron_count**: List defining neurons per layer `[hidden1, hidden2, ..., output]`
- **hidden_layers**: Number of hidden layers
- **epochs**: Training iterations
- **learning_rate**: Step size for gradient descent

### Activation Functions
The implementation uses **Leaky ReLU** for hidden layers by default, which helps prevent the "dying ReLU" problem while maintaining computational efficiency.

### Weight Initialization
- **He Normal**: Default for ReLU-based networks
- **Xavier Normal**: Available for symmetric activations
- **Random Bias**: Small random values to break symmetry

## üìä Performance Considerations

### Numerical Stability
- **Gradient Clipping**: Prevents exploding gradients
- **Epsilon Clipping**: Prevents log(0) in loss calculations
- **Overflow Protection**: Sigmoid function clipped to prevent overflow

### Memory Efficiency
- **In-place Operations**: Minimizes memory allocation
- **Efficient Matrix Operations**: Leverages NumPy's optimized BLAS
- **Best Model Tracking**: Saves only the best performing model

## üéì Educational Value

This implementation is designed to be educational. Each function includes:
- **Mathematical formulation** in docstrings
- **Step-by-step computation** with clear variable names
- **Conceptual explanations** of why each operation is performed
- **Numerical stability considerations**

Perfect for:
- Students learning neural networks
- Researchers implementing custom architectures
- Anyone wanting to understand the mathematical foundations of deep learning

### üóÇÔ∏è File Descriptions

- **`neural_network_model.py`**: Core implementation containing all neural network functions
- **`iris_classification.py`**: Complete classification example using the famous Iris dataset
- **`housing_data_regression.py`**: Regression example demonstrating continuous value prediction
- **`data/`**: Directory containing sample datasets for testing and examples

## üêõ Known Limitations

1. **No GPU Support**: CPU-only implementation
2. **Basic Optimization**: Only vanilla gradient descent
3. **Limited Regularization**: No dropout, batch normalization, or weight decay
4. **Single Batch**: Processes entire dataset at once (no mini-batching)
5. **Fixed Architecture**: No skip connections or advanced architectures

## ü§ù Contributing

This is an educational implementation. Contributions that improve:
- Mathematical clarity
- Code documentation
- Educational examples
- Performance optimizations

are welcome!

## üìÑ License

MIT License - Feel free to use this for educational purposes, research, or as a foundation for your own implementations.

---

**Built with ‚ù§Ô∏è and Pure Mathematics**

*"The best way to understand neural networks is to build them from scratch."*
